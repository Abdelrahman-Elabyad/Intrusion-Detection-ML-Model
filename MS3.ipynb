{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution (Counts):\n",
      "class\n",
      "0    13449\n",
      "1    11743\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class Distribution (Proportions):\n",
      "class\n",
      "0    0.53386\n",
      "1    0.46614\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Dataset is already balanced. No action needed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from scipy.stats import norm\n",
    "from scipy import stats\n",
    "from sklearn.utils import resample\n",
    "import warnings\n",
    "# Load the dataset\n",
    "file_path = \"Train_data.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "distributions = [\n",
    "    'norm', 'uniform', 'expon', 'pareto',  'cauchy', 'triang',\n",
    "    'weibull_min', 'weibull_max', 'gamma', 'beta', 'chi2', 'fisk', \n",
    "    't', 'genextreme', 'gumbel_r', 'gumbel_l', 'laplace', 'rayleigh',\n",
    "    'invgauss', 'halfnorm', 'exponpow', 'exponnorm', \n",
    "    'invweibull', 'nakagami', 'johnsonsu', 'genlogistic', 'dweibull'\n",
    "]\n",
    "# Split data into features and target\n",
    "target_column = 'class'\n",
    "categorical_columns = data.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_columns = data.select_dtypes(include=['int64', 'float64']).columns.difference([target_column]).tolist()\n",
    "\n",
    "# Encode target variable\n",
    "data[target_column] = data[target_column].apply(lambda x: 1 if x == 'anomaly' else 0)\n",
    "\n",
    "# Remove columns with zero variance\n",
    "for column in numerical_columns:\n",
    "    if data[column].std() == 0:\n",
    "        numerical_columns.remove(column)\n",
    "\n",
    "# Step to handle class imbalance\n",
    "class_counts = data[target_column].value_counts()\n",
    "class_proportions = data[target_column].value_counts(normalize=True)\n",
    "\n",
    "print(\"Class Distribution (Counts):\")\n",
    "print(class_counts)\n",
    "\n",
    "print(\"\\nClass Distribution (Proportions):\")\n",
    "print(class_proportions)\n",
    "\n",
    "# Check for imbalance\n",
    "imbalance_threshold = 0.7  # Threshold to determine if dataset is imbalanced\n",
    "max_proportion = class_proportions.max()\n",
    "\n",
    "if max_proportion > imbalance_threshold:\n",
    "    print(\"\\nClass imbalance detected. Balancing the dataset...\")\n",
    "    majority_class = class_counts.idxmax()\n",
    "    minority_class = class_counts.idxmin()\n",
    "\n",
    "    # Separate majority and minority classes\n",
    "    majority_data = data[data[target_column] == majority_class]\n",
    "    minority_data = data[data[target_column] == minority_class]\n",
    "\n",
    "    # Resample the minority class to match the majority class size\n",
    "    balanced_minority_data = resample(\n",
    "        minority_data,\n",
    "        replace=True,  # Sample with replacement\n",
    "        n_samples=majority_data.shape[0],  # Match the majority class size\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Combine majority class with the balanced minority class\n",
    "    data = pd.concat([majority_data, balanced_minority_data])\n",
    "    print(\"\\nAfter balancing:\")\n",
    "    print(data[target_column].value_counts())\n",
    "else:\n",
    "    print(\"\\nDataset is already balanced. No action needed.\")\n",
    "\n",
    "# Train-test split\n",
    "train_data, test_data = train_test_split(data, test_size=0.3, random_state=42, stratify=data[target_column])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Results (No Condition):\n",
      "count: {'Best Fit Distribution': 'cauchy', 'Parameters': (np.float64(6.138683491762478), np.float64(13.649309521830268))}\n",
      "diff_srv_rate: {'Best Fit Distribution': 'exponnorm', 'Parameters': (np.float64(2610.748598995944), np.float64(-8.283971074110917e-05), np.float64(2.3721966723411414e-05))}\n",
      "dst_bytes: {'Best Fit Distribution': 'halfnorm', 'Parameters': (np.int64(0), np.float64(88897.56062666258))}\n",
      "dst_host_count: {'Best Fit Distribution': 'weibull_max', 'Parameters': (np.float64(0.9701176317515333), np.float64(255.00000000000006), np.float64(96.71149741250534))}\n",
      "dst_host_diff_srv_rate: {'Best Fit Distribution': 'exponnorm', 'Parameters': (np.float64(2926.077808050304), np.float64(-9.678028023168771e-05), np.float64(2.8041023801918117e-05))}\n",
      "dst_host_rerror_rate: {'Best Fit Distribution': 'exponnorm', 'Parameters': (np.float64(3469.304703144013), np.float64(-0.000123036570600791), np.float64(3.371644043134232e-05))}\n",
      "dst_host_same_src_port_rate: {'Best Fit Distribution': 'pareto', 'Parameters': (np.float64(113780278.20806159), np.float64(-16777216.0), np.float64(16777215.999999998))}\n",
      "dst_host_same_srv_rate: {'Best Fit Distribution': 'dweibull', 'Parameters': (np.float64(5.575065918035815), np.float64(0.5157208578136678), np.float64(0.4666003501847473))}\n",
      "dst_host_serror_rate: {'Best Fit Distribution': 'dweibull', 'Parameters': (np.float64(35.72922384862575), np.float64(0.4974666842001468), np.float64(0.4981008922068878))}\n",
      "dst_host_srv_count: {'Best Fit Distribution': 'invgauss', 'Parameters': (np.float64(4.436465245636905), np.float64(-4.466743873714172), np.float64(26.942542154802446))}\n",
      "dst_host_srv_diff_host_rate: {'Best Fit Distribution': 'exponnorm', 'Parameters': (np.float64(3438.728665256118), np.float64(-3.408126158172228e-05), np.float64(9.222883416355779e-06))}\n",
      "dst_host_srv_rerror_rate: {'Best Fit Distribution': 'exponnorm', 'Parameters': (np.float64(3550.4716969376113), np.float64(-0.00013116183373189318), np.float64(3.316107642278462e-05))}\n",
      "dst_host_srv_serror_rate: {'Best Fit Distribution': 'dweibull', 'Parameters': (np.float64(65.67117178296957), np.float64(0.49806284004222995), np.float64(0.4990341203661294))}\n",
      "duration: {'Best Fit Distribution': 'laplace', 'Parameters': (np.float64(0.0), np.float64(305.05410447761193))}\n",
      "hot: {'Best Fit Distribution': 'genlogistic', 'Parameters': (np.float64(2191.649304190947), np.float64(-1.5194538191543652), np.float64(0.19798226597604285))}\n",
      "is_guest_login: {'Best Fit Distribution': 'exponnorm', 'Parameters': (np.float64(4460.141055286239), np.float64(-7.437733492236431e-06), np.float64(2.0536689273857413e-06))}\n",
      "is_host_login: {'Best Fit Distribution': 'norm', 'Parameters': (np.float64(0.0), np.float64(0.0))}\n",
      "land: {'Best Fit Distribution': 'rayleigh', 'Parameters': (-0.008909415131606764, np.float64(0.008949374352082708))}\n",
      "logged_in: {'Best Fit Distribution': 'exponnorm', 'Parameters': (np.float64(4391.123682328325), np.float64(-0.00033964948050284414), np.float64(8.94176415550085e-05))}\n",
      "num_access_files: {'Best Fit Distribution': 'rayleigh', 'Parameters': (-0.09832048112118258, np.float64(0.1006057824518807))}\n",
      "num_compromised: {'Best Fit Distribution': 'rayleigh', 'Parameters': (-10.398734771933247, np.float64(10.522386183256796))}\n",
      "num_failed_logins: {'Best Fit Distribution': 'rayleigh', 'Parameters': (-0.04539199722559739, np.float64(0.046003756722461285))}\n",
      "num_file_creations: {'Best Fit Distribution': 'weibull_max', 'Parameters': (np.float64(1724851.6431645001), np.float64(282767.44539846433), np.float64(282767.4443537077))}\n",
      "num_outbound_cmds: {'Best Fit Distribution': 'norm', 'Parameters': (np.float64(0.0), np.float64(0.0))}\n",
      "num_root: {'Best Fit Distribution': 'rayleigh', 'Parameters': (-11.482964274862976, np.float64(11.617289622897177))}\n",
      "num_shells: {'Best Fit Distribution': 'halfnorm', 'Parameters': (np.int64(0), np.float64(0.0189012240829087))}\n",
      "rerror_rate: {'Best Fit Distribution': 'exponnorm', 'Parameters': (np.float64(3124.1627046222384), np.float64(-0.00014225701918522125), np.float64(3.77774315667677e-05))}\n",
      "root_shell: {'Best Fit Distribution': 'halfnorm', 'Parameters': (np.int64(0), np.float64(0.03934603552168147))}\n",
      "same_srv_rate: {'Best Fit Distribution': 'dweibull', 'Parameters': (np.float64(9.471230050127009), np.float64(0.5378717564520847), np.float64(0.4705556782466926))}\n",
      "serror_rate: {'Best Fit Distribution': 'dweibull', 'Parameters': (np.float64(28.47016314280179), np.float64(0.5078522749167875), np.float64(0.5039494083496978))}\n",
      "src_bytes: {'Best Fit Distribution': 'rayleigh', 'Parameters': (-2406085.4983368665, np.float64(2420606.7965674466))}\n",
      "srv_count: {'Best Fit Distribution': 'expon', 'Parameters': (1.0, 26.69875357256272)}\n",
      "srv_diff_host_rate: {'Best Fit Distribution': 'exponnorm', 'Parameters': (np.float64(2930.6284656763255), np.float64(-0.0001147425153754348), np.float64(3.25132492925973e-05))}\n",
      "srv_rerror_rate: {'Best Fit Distribution': 'exponnorm', 'Parameters': (np.float64(2624.365931479135), np.float64(-0.0001648025522331234), np.float64(4.555604020429154e-05))}\n",
      "srv_serror_rate: {'Best Fit Distribution': 'dweibull', 'Parameters': (np.float64(36.246221578897334), np.float64(0.5062062082332586), np.float64(0.5033787698074201))}\n",
      "su_attempted: {'Best Fit Distribution': 'weibull_max', 'Parameters': (np.float64(28389235452.395073), np.float64(317178923.70253205), np.float64(317178923.70252293))}\n",
      "urgent: {'Best Fit Distribution': 'rayleigh', 'Parameters': (-0.006300157936950673, np.float64(0.006320098997027263))}\n",
      "wrong_fragment: {'Best Fit Distribution': 'laplace', 'Parameters': (np.float64(0.0), np.float64(0.023737694506192442))}\n",
      "\n",
      "Conditioned on 'Normal':\n",
      "\n",
      "Conditioned on 'Anomaly':\n"
     ]
    }
   ],
   "source": [
    "# Function to fit PDFs and return MSE + parameters\n",
    "def fit_distributions(data_column, distributions):\n",
    "    results = {}\n",
    "    x = np.linspace(min(data_column), max(data_column), 100)\n",
    "\n",
    "    for dist_name in distributions:\n",
    "        try:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\", RuntimeWarning)  # Suppress warnings\n",
    "                dist = getattr(stats, dist_name)\n",
    "                params = dist.fit(data_column)\n",
    "\n",
    "                # Validate parameters\n",
    "                if not all(np.isfinite(params)):\n",
    "                    raise ValueError(f\"Invalid parameters for {dist_name}\")\n",
    "\n",
    "                # Calculate PDF and MSE\n",
    "                pdf_fitted = dist.pdf(x, *params)\n",
    "                mse = np.mean((pdf_fitted - np.histogram(data_column, bins=100, density=True)[0]) ** 2)\n",
    "\n",
    "                # Store MSE and parameters\n",
    "                results[dist_name] = {'MSE': mse, 'Parameters': params}\n",
    "\n",
    "        except (ValueError, RuntimeError, OverflowError, TypeError):\n",
    "            pass  # Skip invalid distributions\n",
    "\n",
    "    return results\n",
    "\n",
    "# Summarize results for all conditions\n",
    "def summarize_results_by_condition(data):\n",
    "    distributions = [\n",
    "    'norm', 'uniform', 'expon', 'pareto',  'cauchy', 'triang',\n",
    "    'weibull_min', 'weibull_max', 'gamma', 'beta', 'chi2', 'fisk', \n",
    "    't', 'genextreme', 'gumbel_r', 'gumbel_l', 'laplace', 'rayleigh',\n",
    "    'invgauss', 'halfnorm', 'exponpow', 'exponnorm', \n",
    "    'invweibull', 'nakagami', 'johnsonsu', 'genlogistic', 'dweibull'\n",
    "]\n",
    "\n",
    "    numerical_columns = data.select_dtypes(include=['float64', 'int64']).columns.difference(['class'])\n",
    "\n",
    "    # Create dictionaries for each case\n",
    "    overall_results = {}\n",
    "    normal_results = {}\n",
    "    anomaly_results = {}\n",
    "\n",
    "    for column in numerical_columns:\n",
    "        col_data = data[column].dropna()\n",
    "        col_data_normal = data.loc[data['class'] == 'normal', column].dropna()\n",
    "        col_data_anomaly = data.loc[data['class'] == 'anomaly', column].dropna()\n",
    "\n",
    "        # No condition: Fit all data\n",
    "        if not col_data.empty:\n",
    "            fit_results = fit_distributions(col_data, distributions)\n",
    "            if fit_results:\n",
    "                best_fit_name = min(fit_results, key=lambda x: fit_results[x]['MSE'])\n",
    "                overall_results[column] = {\n",
    "                    'Best Fit Distribution': best_fit_name,\n",
    "                    'Parameters': fit_results[best_fit_name]['Parameters']\n",
    "                }\n",
    "\n",
    "        # Conditioned on 'normal'\n",
    "        if not col_data_normal.empty:\n",
    "            fit_results = fit_distributions(col_data_normal, distributions)\n",
    "            if fit_results:\n",
    "                best_fit_name = min(fit_results, key=lambda x: fit_results[x]['MSE'])\n",
    "                normal_results[column] = {\n",
    "                    'Best Fit Distribution': best_fit_name,\n",
    "                    'Parameters': fit_results[best_fit_name]['Parameters']\n",
    "                }\n",
    "\n",
    "        # Conditioned on 'anomaly'\n",
    "        if not col_data_anomaly.empty:\n",
    "            fit_results = fit_distributions(col_data_anomaly, distributions)\n",
    "            if fit_results:\n",
    "                best_fit_name = min(fit_results, key=lambda x: fit_results[x]['MSE'])\n",
    "                anomaly_results[column] = {\n",
    "                    'Best Fit Distribution': best_fit_name,\n",
    "                    'Parameters': fit_results[best_fit_name]['Parameters']\n",
    "                }\n",
    "\n",
    "    return overall_results, normal_results, anomaly_results\n",
    "\n",
    "# Execute and get results\n",
    "overall_results, normal_results, anomaly_results = summarize_results_by_condition(data)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nOverall Results (No Condition):\")\n",
    "for column, details in overall_results.items():\n",
    "    print(f\"{column}: {details}\")\n",
    "\n",
    "print(\"\\nConditioned on 'Normal':\")\n",
    "for column, details in normal_results.items():\n",
    "    print(f\"{column}: {details}\")\n",
    "\n",
    "print(\"\\nConditioned on 'Anomaly':\")\n",
    "for column, details in anomaly_results.items():\n",
    "    print(f\"{column}: {details}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance:\n",
      "Accuracy: 0.94\n",
      "Precision: 0.91\n",
      "Recall: 0.96\n"
     ]
    }
   ],
   "source": [
    "# Add a small value to std to avoid division by zero\n",
    "def calculate_pdf(column, values, condition):\n",
    "    mean = column[condition].mean()\n",
    "    std = column[condition].std() + 1e-5  # Adding a small value to avoid zero std\n",
    "    return norm.pdf(values, mean, std)\n",
    "\n",
    "def calculate_pmf(column, values, condition):\n",
    "    pmf = column[condition].value_counts(normalize=True, dropna=False)\n",
    "    pmf += 1e-5  # Laplace smoothing\n",
    "    pmf /= pmf.sum()  # Re-normalize\n",
    "    return [pmf.get(v, 1e-5) for v in values]\n",
    "\n",
    "# Compute prior probabilities after balancing\n",
    "class_weights = train_data[target_column].value_counts(normalize=True)\n",
    "prior_anomaly = class_weights[1]\n",
    "prior_no_anomaly = class_weights[0]\n",
    "\n",
    "def predict(test_data, train_data, priors, numerical_columns, categorical_columns):\n",
    "    anomaly_probs = []\n",
    "    no_anomaly_probs = []\n",
    "\n",
    "    for _, row in test_data.iterrows():\n",
    "        likelihood_anomaly = 1.0\n",
    "        likelihood_no_anomaly = 1.0\n",
    "\n",
    "        for feature in numerical_columns:\n",
    "            anomaly_pdf = calculate_pdf(train_data[feature], [row[feature]], train_data[target_column] == 1)\n",
    "            no_anomaly_pdf = calculate_pdf(train_data[feature], [row[feature]], train_data[target_column] == 0)\n",
    "            likelihood_anomaly *= anomaly_pdf[0]\n",
    "            likelihood_no_anomaly *= no_anomaly_pdf[0]\n",
    "\n",
    "        for feature in categorical_columns:\n",
    "            anomaly_pmf = calculate_pmf(train_data[feature], [row[feature]], train_data[target_column] == 1)\n",
    "            no_anomaly_pmf = calculate_pmf(train_data[feature], [row[feature]], train_data[target_column] == 0)\n",
    "            likelihood_anomaly *= anomaly_pmf[0]\n",
    "            likelihood_no_anomaly *= no_anomaly_pmf[0]\n",
    "\n",
    "        anomaly_prob = likelihood_anomaly * priors['anomaly']\n",
    "        no_anomaly_prob = likelihood_no_anomaly * priors['no_anomaly']\n",
    "\n",
    "        anomaly_probs.append(anomaly_prob)\n",
    "        no_anomaly_probs.append(no_anomaly_prob)\n",
    "\n",
    "    return np.array(anomaly_probs), np.array(no_anomaly_probs)\n",
    "\n",
    "priors = {'anomaly': prior_anomaly, 'no_anomaly': prior_no_anomaly}\n",
    "anomaly_probs, no_anomaly_probs = predict(test_data, train_data, priors, numerical_columns, categorical_columns)\n",
    "\n",
    "predictions = (anomaly_probs > no_anomaly_probs).astype(int)\n",
    "accuracy = accuracy_score(test_data[target_column], predictions)\n",
    "precision = precision_score(test_data[target_column], predictions)\n",
    "recall = recall_score(test_data[target_column], predictions)\n",
    "\n",
    "print(\"\\nModel Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['class'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_score, recall_score\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Identifying categorical features that need encoding, excluding the target 'class'\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m categorical_features_to_encode \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mobject\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclass\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# One-hot encoding these features\u001b[39;00m\n\u001b[0;32m     11\u001b[0m encoder \u001b[38;5;241m=\u001b[39m OneHotEncoder(sparse_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)  \n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['class'] not found in axis\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "# Identifying categorical features that need encoding, excluding the target 'class'\n",
    "categorical_features_to_encode = data.select_dtypes(include=['object']).columns.drop('class')\n",
    "\n",
    "# One-hot encoding these features\n",
    "encoder = OneHotEncoder(sparse_output=False)  \n",
    "encoded_features = encoder.fit_transform(data[categorical_features_to_encode])\n",
    "encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out())\n",
    "\n",
    "# Merging the encoded features back with the dataset\n",
    "data_prepared = data.drop(columns=categorical_features_to_encode)\n",
    "data_prepared = pd.concat([data_prepared, encoded_df], axis=1)\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X = data_prepared.drop('class', axis=1)\n",
    "y = data_prepared['class'].apply(lambda x: 1 if x == 'normal' else 0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Training Gaussian Naive Bayes\n",
    "gaussian_nb = GaussianNB()\n",
    "gaussian_nb.fit(X_train, y_train)\n",
    "y_pred_gaussian = gaussian_nb.predict(X_test)\n",
    "\n",
    "# Training Multinomial Naive Bayes\n",
    "multinomial_nb = MultinomialNB()\n",
    "multinomial_nb.fit(X_train, y_train)\n",
    "y_pred_multinomial = multinomial_nb.predict(X_test)\n",
    "\n",
    "# Training Bernoulli Naive Bayes\n",
    "bernoulli_nb = BernoulliNB()\n",
    "bernoulli_nb.fit(X_train, y_train)\n",
    "y_pred_bernoulli = bernoulli_nb.predict(X_test)\n",
    "\n",
    "# Collecting performance metrics for each model\n",
    "gaussian_acc = accuracy_score(y_test, y_pred_gaussian)\n",
    "multinomial_acc = accuracy_score(y_test, y_pred_multinomial)\n",
    "bernoulli_acc = accuracy_score(y_test, y_pred_bernoulli)\n",
    "\n",
    "gaussian_prec = precision_score(y_test, y_pred_gaussian)\n",
    "multinomial_prec = precision_score(y_test, y_pred_multinomial)\n",
    "bernoulli_prec = precision_score(y_test, y_pred_bernoulli)\n",
    "\n",
    "gaussian_recall = recall_score(y_test, y_pred_gaussian)\n",
    "multinomial_recall = recall_score(y_test, y_pred_multinomial)\n",
    "bernoulli_recall = recall_score(y_test, y_pred_bernoulli)\n",
    "\n",
    "# Printing the results\n",
    "print(\"Gaussian Naive Bayes:\")\n",
    "print(\"Accuracy:\", gaussian_acc)\n",
    "print(\"Precision:\", gaussian_prec)\n",
    "print(\"Recall:\", gaussian_recall)\n",
    "\n",
    "print(\"\\nMultinomial Naive Bayes:\")\n",
    "print(\"Accuracy:\", multinomial_acc)\n",
    "print(\"Precision:\", multinomial_prec)\n",
    "print(\"Recall:\", multinomial_recall)\n",
    "\n",
    "print(\"\\nBernoulli Naive Bayes:\")\n",
    "print(\"Accuracy:\", bernoulli_acc)\n",
    "print(\"Precision:\", bernoulli_prec)\n",
    "print(\"Recall:\", bernoulli_recall)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
